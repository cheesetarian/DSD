{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prep output files to examine how much variability exists in the OceanRain data (OceanRain-R)\n",
    "# NOW using machine learning approach that exists in dsd/dj\n",
    "# bugs fixed 2/5/18 DD, now outputs numpy arrays of all lats, lons, and counts (within some bounds, rain-only)\n",
    "# -- this is essentially pre-processing code for dsd/dj/OceanClustering, gmm_solo\n",
    "#  updated DD 24/08/18 to keep total counts the same despite weighting by volume\n",
    "# changed a few times aug/sept to use the OR \"M\" data instead, so as to pass along values per mm,\n",
    "#  normalized in different ways that should be specified by output filename\n",
    "# NOTE: inclusion of W files takes longer to read in and then subset... comment out \n",
    "#  if needs be, i.e. if there's no need for the ancillary vars\n",
    "\n",
    "# cleaned up a little dec 2018... much of code isn't used anymore, but still useful for reading\n",
    "#  through all OR ship-specific files and spitting out more easily used npy arrays for each variable of interest\n",
    "\n",
    "dir = '/home/dudavid/Dendrite/Dendrite/UserAreas/Dave/DSD/'\n",
    "#import h5py\n",
    "from netCDF4 import Dataset # these data work with both h5py and netCDF4 libs?\n",
    "import pygal\n",
    "from pygal.style import DarkColorizedStyle,DarkSolarizedStyle,CleanStyle\n",
    "from decimal import Decimal # display No in sci notation\n",
    "import glob\n",
    "from subprocess import call\n",
    "\n",
    "tout = 'MSs3' # title out, used 'VS4' before for volume-weighted (R^3) and smoothed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "def pca2(data, pc_count = None):\n",
    "    pca = PCA(n_components = npc) # can choose solver too, etc.\n",
    "    return pca.fit_transform(data), pca.explained_variance_ratio_\n",
    "\n",
    "npc = 6 # set number of PCs to solve for here\n",
    "# input array should be 2 dimensional\n",
    "#PCs, varex = pca2(np.transpose(darseas2d),npc)\n",
    "\n",
    "def normalized(a, axis=-1, order=2):\n",
    "    l2 = np.atleast_1d(np.linalg.norm(a, order, axis))\n",
    "    l2[l2==0] = 1\n",
    "    return a / np.expand_dims(l2, axis)\n",
    "\n",
    "def smooth(y, box_pts):\n",
    "    box = np.ones(box_pts)/box_pts\n",
    "    y_smooth = np.convolve(y[1:], box, mode='same')\n",
    "    y_smooth = np.append(y[0],y_smooth) # to leave first point unmolested\n",
    "    return y_smooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The-Wor The-Wor\n",
      "1180 The-Wor\n",
      "Investi Investi\n",
      "10242 Investi\n",
      "Maria-S Maria-S\n",
      "9392 Maria-S\n",
      "Meteor_ Meteor_\n",
      "10461 Meteor_\n",
      "Polarst Polarst\n",
      "32870 Polarst\n",
      "Roger-R Roger-R\n",
      "4750 Roger-R\n",
      "SonneI_ SonneI_\n",
      "3277 SonneI_\n",
      "SonneII SonneII\n",
      "19944 SonneII\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'andstophere' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-6bf1c1be9135>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dj/data/alldtd'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mtout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malldt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dj/data/allepochd'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mtout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mallepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m \u001b[0mandstophere\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;31m# normalize in the vertical by counts, integrate to 1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'andstophere' is not defined"
     ]
    }
   ],
   "source": [
    "filist = glob.glob(dir+'*_M_*.nc') # raw (R) files -- SWITching TO M FILES\n",
    "allct = []\n",
    "alllo = []\n",
    "allla = []\n",
    "alldm,allrr,allnw = [], [], []\n",
    "allku,allka, allmu,alldate = [], [], [], []\n",
    "\n",
    "wfilist = glob.glob(dir+'*_W_*.nc') # adding W (water) files\n",
    "alldt, allss, allepoch = [],[],[]\n",
    "\n",
    "bn = np.load('oceanrain_binsizes.npy') # bin sizes in float array, copied to local dir from Dendrite DSD dir\n",
    "# NOTE thesee are particle DIAMETERS, not radii\n",
    "nplim = 50 # number particles limit to include data point\n",
    "bnz = 80 # there are 128 (going up to 22mm!) so don't need them all, surely\n",
    "startb = 12 # bin to start (most early ones have zero counts!)\n",
    "smo = 5 # smoothing distance in OR bins\n",
    "allcv = np.zeros([bnz,0])\n",
    "allcr = np.zeros([bnz,0])\n",
    "allcs = np.zeros([bnz,0])\n",
    "allcw = np.zeros([bnz,0])\n",
    "\n",
    "for f in wfilist[:]:\n",
    "    tit = f[68:75]\n",
    "    dat= Dataset(f)\n",
    "    # vars used to screen data points for quality and rain:\n",
    "    \n",
    "for f in range(len(filist)): #filist[:]:\n",
    "    ef1 = filist[f]\n",
    "    ef2 = wfilist[f]\n",
    "    tit = ef1[68:75]\n",
    "    print(ef1[68:75],ef2[68:75])\n",
    "    dat= Dataset(ef1)\n",
    "    datw=Dataset(ef2) # W files\n",
    "    # vars used to screen data points for quality and rain:\n",
    "    pop= dat['probability_for_rain'][:]\n",
    "    pf = dat['precip_flag'][:] # 0 = rain\n",
    "    dm = dat['mass_weighted_mean_diameter_of_normalized_gamma'][:] #[rsub][psub]\n",
    "    npar = dat['number_of_particles'][:]\n",
    "    \n",
    "    rsub = np.logical_and(pf==0, npar>nplim) #pop==1.0, npar>50)\n",
    "    #rsub =  pf == 0   #, npar>nplim) #pop==1.0, npar>50)\n",
    "    psub = pop[rsub] == 1.0  # ), dm[rsub] > 0.0)\n",
    "    dsub = dm[rsub][psub] > 0.0\n",
    "    \n",
    "    lo = np.array(dat['longitude'])[rsub][psub][dsub]\n",
    "    la = np.array(dat['latitude'])[rsub][psub][dsub]\n",
    "    Nw = np.array(dat['intercept_of_normalized_gamma'])[rsub][psub][dsub]\n",
    "    rr = np.array(dat['ODM470_precipitation_rate_R'])[rsub][psub][dsub]\n",
    "    ku = np.array(dat['Ku_band_reflectivity'])[rsub][psub][dsub]\n",
    "    ka = np.array(dat['Ka_band_reflectivity'])[rsub][psub][dsub]\n",
    "    mu = np.array(dat['shape_parameter_of_normalized_gamma'])[rsub][psub][dsub]\n",
    "    date= np.array(dat['date_UT'])[rsub][psub][dsub]\n",
    "    dm = dm[rsub][psub][dsub]\n",
    "    #npar = np.array(dat['number_of_particles'])[rsub][psub][dsub]\n",
    "    nr = np.size(lo)\n",
    "    print( nr, tit)\n",
    "    #print('NPAR INFO: ',info(npar))\n",
    "    \n",
    "    # adding W variables:\n",
    "    time_m, time_w = dat['unix_epoch'][:], datw['unix_epoch'][:] # need to match up since M is subset of W!\n",
    "    inds = [a in time_m for a in time_w] # indices for M in W data (unix time should be unique!)\n",
    "    dt = np.array(datw['dew_point_temperature'][inds])[rsub][psub][dsub]\n",
    "    ss = np.array(datw['sea_surface_temperature'][inds])[rsub][psub][dsub]\n",
    "    \n",
    "    rallcts = np.zeros([bnz,nr])  # all counts\n",
    "    sallcts = np.zeros([bnz,nr])  # smoothed all counts\n",
    "    vallcts = np.zeros([bnz,nr])  # weighted by volume/mass (R^3) [then smoothed]\n",
    "    wallcts = np.zeros([bnz,nr])  # weighted by volume/mass (R^3)\n",
    "\n",
    "    for b in range(bnz):\n",
    "        # values for bins given are size CENTERs, as said in README of documentation\n",
    "        bz = np.array(dat['bin'+str(b+startb+1)])[rsub][psub][dsub] # integer counts, array size is ntime\n",
    "        rallcts[b,:] = bz[:]\n",
    "        wallcts[b,:] = bz[:]*((bn[b+startb]*0.5)**3) # weighting raw counts by r^3 (volume/rwc)\n",
    "        #wallcts[b,:] =bz[:]*(bn[b+startb]**3)*(bn[b+startb]*2)**0.67 # counts weighted by volume/mass/rwc * velocity\n",
    "    \n",
    "    for n in range(nr):\n",
    "        # smooth cts with moving boxcar, but don't change total number of counts\n",
    "        ## normalizing below anyway, so non-int count values are fine\n",
    "        vallcts[:,n] = smooth(wallcts[:,n],smo) * (np.sum(wallcts[:,n])/np.sum(smooth(wallcts[:,n],smo)))\n",
    "        # should conserve total mass (instead of counts) for each raining minute\n",
    "        sallcts[:,n] = smooth(rallcts[:,n],smo) * (np.sum(rallcts[:,n])/np.sum(smooth(rallcts[:,n],smo)))\n",
    "    \n",
    "    #tot_drops = np.sum(rallcts, axis=0)\n",
    "    #gu = np.array(np.where(tot_drops > nplim))[0,:] \n",
    "    #gu = np.array(np.where(npar > nplim))[0,:] \n",
    "    # screen out points with few drops -- CURRENTLY DOESNT DO ANYTHING FOR M RUNS>>>\n",
    "    #tot_drops_g = tot_drops[gu]\n",
    "    rallcts_g = rallcts #[:,gu] # save raw counts for eliminating obs with few drops\n",
    "    vallcts_g = vallcts #[:,gu]\n",
    "    wallcts_g = wallcts #[:,gu]\n",
    "    sallcts_g = sallcts #[:,gu]\n",
    "    #print(np.shape(tot_drops),np.shape(sallcts_g))\n",
    "    \n",
    "    alllo.extend(lo)#[gu])    \n",
    "    allla.extend(la)#[gu])    \n",
    "    alldm.extend(dm)#[gu])\n",
    "    allrr.extend(rr)#[gu])\n",
    "    allnw.extend(Nw)#[gu])\n",
    "    allku.extend(ku)#[gu])\n",
    "    allka.extend(ka)#[gu])\n",
    "    allmu.extend(mu)#[gu])\n",
    "    \n",
    "    allss.extend(ss)\n",
    "    alldt.extend(dt)\n",
    "    alldate.extend(date)\n",
    "    allepoch.extend(time_m) # saving unix epoch\n",
    "\n",
    "    #nrn = np.size(tot_drops_g)\n",
    "    allcv = np.hstack([allcv,vallcts_g])\n",
    "    allcr = np.hstack([allcr,rallcts_g])\n",
    "    allcs = np.hstack([allcs,sallcts_g])\n",
    "    allcw = np.hstack([allcw,wallcts_g])\n",
    "        #allcw = np.append(allcw, np.resize(wallcts_g[:,n],[bnz,1]), axis=1)\n",
    "    \n",
    "np.save('dj/data/alllatd'+tout, np.array(allla)) # one numpy dump for all (ship) files\n",
    "np.save('dj/data/alllond'+tout, np.array(alllo))\n",
    "np.save('dj/data/allrrd'+tout, np.array(allrr))\n",
    "np.save('dj/data/allnwd'+tout, np.array(allnw))\n",
    "np.save('dj/data/alldmd'+tout, np.array(alldm))\n",
    "np.save('dj/data/allkud'+tout, np.array(allku))\n",
    "np.save('dj/data/allkad'+tout, np.array(allka))\n",
    "np.save('dj/data/allmud'+tout, np.array(allmu))\n",
    "np.save('dj/data/alldatd'+tout,np.array(alldate))\n",
    "np.save('dj/data/allssd'+tout, np.array(allss))\n",
    "np.save('dj/data/alldtd'+tout, np.array(alldt))\n",
    "np.save('dj/data/allepochd'+tout, np.array(allepoch))\n",
    "\n",
    "# normalize in the vertical by counts, integrate to 1.0\n",
    "vnallcout = allcv / np.sum(allcv, axis=0) # normalized, vol-weighted, smoothed\n",
    "wallcout  = allcw / np.sum(allcw, axis=0) # normalized, vol-weighted\n",
    "sallcout  = allcs / np.sum(allcs, axis=0) # normalized, smoothed (not weighted!)\n",
    "print(np.sum(vnallcout[:,174])) # should be 1!\n",
    "\n",
    "# trying to normalize by RWC:\n",
    "mpy = bn[startb:(startb+bnz+1)]\n",
    "dD = [mpy[n+1]-mpy[n] for n in range(len(mpy[:bnz]))] # bin width [diameter, mm]\n",
    "norm_fac = 1000.0 * (4/3 * np.pi * (0.001*mpy[:-1]*.5)**3) * dD\n",
    "# density * volume * width of bin (since per mm from M data) [units: kg/m3 * m3... ]\n",
    "rwc_norm = np.array([norm_fac * allcs[:,s] / np.sum(norm_fac * allcs[:,s]) for s in range(len(alllo))]).transpose()\n",
    "\n",
    "np.save('dj/data/normallcts'+tout+'.'+str(bnz),vnallcout) # normalized to 1.0 sum \n",
    "#np.save('dj/data/normallcts'+tout+'.'+str(bnz),wallcout) # weighted, normalized but not smoothed \n",
    "np.save('dj/data/snallcts'+tout+'.'+str(bnz),sallcout) # normalized, smoothed, not weighted \n",
    "np.save('dj/data/vallcts'+tout+'.'+str(bnz),allcv) # not normalized (but still volume-weighted, smoothed)\n",
    "np.save('dj/data/rallcts'+tout+'.'+str(bnz),allcr) # not normalized, just raw counts\n",
    "np.save('dj/data/sallcts'+tout+'.'+str(bnz),allcs) # not normalized, just smoothed raw counts\n",
    "np.save('dj/data/wallcts'+tout+'.'+str(bnz),allcw) # not normalized, just weighted raw counts\n",
    "np.save('dj/data/rwallcts'+tout+'.'+str(bnz),rwc_norm) # RWC-normalized, (smoothed) raw counts\n",
    "print(np.shape(vnallcout),np.shape(allla))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(info(alldat))\n",
    "\n",
    "print(info(allku))\n",
    "print(info(allka))\n",
    "print(info(allmu))\n",
    "print(rwc_norm[:40,5])\n",
    "print(rwc_norm[:40,259])\n",
    "print(rwc_norm[:40,9990+300])\n",
    "print(rwc_norm[:40,9990-300])\n",
    "print(np.sum(rwc_norm[:,2159]))\n",
    "print(np.sum(rwc_norm[:,259]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cz = 9990\n",
    "rg = 40\n",
    "#print('raw ',allcr[:rg,cz])\n",
    "#print('raww ',(bn[startb:(startb+rg)]**3)*allcr[:rg,cz]) # should be identical!\n",
    "#print('wghtd ',allcw[:rg,cz])\n",
    "#print('smoothd ',allcs[:rg,cz])\n",
    "#print('wghtd smthd ',allcv[:rg,cz])\n",
    "#print('vol smoothd normd ',vnallcout[:20,0])\n",
    "print('totalz: ',np.sum(allcr[:rg,cz]),np.sum(allcs[:rg,cz]),np.sum(allcw[:rg,cz]),np.sum(allcv[:rg,cz]))\n",
    "plt.figure(figsize=[8,8])\n",
    "#plt.plot(range(rg),    allcr[:rg,cz],'r',label='raw')\n",
    "plt.plot(range(rg),    allcw[:rg,cz],'r--',label='wgtd')\n",
    "#plt.plot(range(rg),    allcs[:rg,cz],'b',label='sm raw')\n",
    "plt.plot(range(rg),    allcv[:rg,cz],'k',label='vol smthd')\n",
    "plt.plot(range(rg),vnallcout[:rg,cz],'g',label='vol smthd norm')\n",
    "plt.plot(range(rg), wallcout[:rg,cz],'c',label='vol norm')\n",
    "plt.plot(range(rg), rwc_norm[:rg,cz],'o',label='RWC norm')\n",
    "plt.plot(range(rg), rwc_norm[:rg,cz+300],'o',label='RWC norm')\n",
    "plt.plot(range(rg), rwc_norm[:rg,cz-300],'o',label='RWC norm')\n",
    "plt.plot(range(rg), rwc_norm[:rg,cz-3800],'o',label='RWC norm')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# how many are in each lat bin, at a few resolutions:\n",
    "bznss = 15.0 # degrees lat\n",
    "maxlat, minlat = 80, -60\n",
    "ny = int((maxlat - minlat)/bznss)\n",
    "latz,dmz = np.array(allla), np.array(alldm)\n",
    "lonz,rrz,nwz = np.array(alllo), np.array(allrr), np.array(allnw)\n",
    "kuz,kaz,muz = np.array(allku), np.array(allka), np.array(allmu)\n",
    "zave = np.zeros(ny)\n",
    "for y in range(ny):\n",
    "    lasub = np.logical_and( latz >= minlat+bznss*y, latz < minlat+(bznss*(y+1)))\n",
    "    print(np.size(latz[lasub]),minlat+bznss*y,minlat+bznss*(y+1))\n",
    "    zave[y] = np.size(latz[lasub])\n",
    "#print(np.mod(zave,float(np.min(zave))))\n",
    "print(np.round(zave/float(np.min(zave))))\n",
    "multiple = np.round(zave/float(np.min(zave))).astype(int) \n",
    "#determine fraction that'll yield approx the same number in each lat range\n",
    "print(multiple) #i.e. take every _th point to get about equal #s per lat bin\n",
    "\n",
    "# shrink size of output arrays relative to smallest lat band for counts:\n",
    "shrnklat,shrnklon = [],[]\n",
    "shrnkrr,shrnknw,shrnkdm = [],[],[]\n",
    "shrnkku,shrnkka,shrnkmu = [],[],[]\n",
    "shrnkctsn = np.zeros([bnz,0])\n",
    "shrnkcts  = np.zeros([bnz,0])\n",
    "for y in range(ny):\n",
    "    lasub = np.logical_and( latz >= minlat+bznss*y, latz < minlat+(bznss*(y+1)))\n",
    "    shrnklat.extend(latz[lasub][::multiple[y]])\n",
    "    shrnklon.extend(lonz[lasub][::multiple[y]])\n",
    "    shrnkrr.extend(rrz[lasub][::multiple[y]])\n",
    "    shrnknw.extend(nwz[lasub][::multiple[y]])\n",
    "    shrnkdm.extend(dmz[lasub][::multiple[y]])\n",
    "    shrnkku.extend(kuz[lasub][::multiple[y]])\n",
    "    shrnkka.extend(kaz[lasub][::multiple[y]])\n",
    "    shrnkmu.extend(muz[lasub][::multiple[y]])\n",
    "    addctsn = vnallcout[:,lasub] # normalized\n",
    "    addcts = allcv[:,lasub] # non-normalized (still weighted)\n",
    "    shrnkctsn = np.append(shrnkctsn, addctsn[:,::multiple[y]], axis=1)\n",
    "    shrnkcts  = np.append(shrnkcts,  addcts[:,::multiple[y]], axis=1)\n",
    "print(np.size(shrnklat), ny*np.min(zave), 'min:',np.min(zave) )\n",
    "\n",
    "# save these shrunken arrays with different filenames to differentiate\n",
    "np.save('dj/data/shrnk.alllatd'+tout, np.array(shrnklat)) # one numpy dump for all files\n",
    "np.save('dj/data/shrnk.alllond'+tout, np.array(shrnklon))\n",
    "np.save( 'dj/data/shrnk.allrrd'+tout, np.array(shrnkrr))\n",
    "np.save( 'dj/data/shrnk.alldmd'+tout, np.array(shrnkdm))\n",
    "np.save( 'dj/data/shrnk.allnwd'+tout, np.array(shrnknw))\n",
    "np.save( 'dj/data/shrnk.allkud'+tout, np.array(shrnkku))\n",
    "np.save( 'dj/data/shrnk.allkad'+tout, np.array(shrnkka))\n",
    "np.save( 'dj/data/shrnk.allmud'+tout, np.array(shrnkmu))\n",
    "\n",
    "# shrunk cts (~equal in each lat bin), V is for volume weighted counts\n",
    "np.save('dj/data/shrnk.normallcts'+tout+'.'+str(bnz),shrnkctsn) # normalized to 1.0 sum (above)\n",
    "np.save('dj/data/shrnk.allcts'+tout+'.'+str(bnz),shrnkcts) # raw counts (but still weighted by R^3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(bn[bnz+startb]) # biggest size bin passed along"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(lkjasdflkjsdf) # not using this section now\n",
    "\n",
    "# take bin closest to Dm, set to counts in bin __ (10?), reorganize counts into X other bins\n",
    "#import rebin\n",
    "from scipy import interpolate, ndimage\n",
    "\n",
    "newbnz = 30 # number new bins... set dm in middle kinda -- set in concert with newbnz frac below\n",
    "bnm = bn[startb:(startb+bnz)]\n",
    "newb = np.arange(bnm[0],bnm[bnz-1],(bnm[10]-bnm[9])*.1) # static, interpolating original cts\n",
    "#print(bnm.size,bnm)\n",
    "#print(alldm[0:30])\n",
    "dm_in = shrnkctsn #allc # was allc before, trying shrunken lat-based array now\n",
    "dout = np.zeros([newbnz,np.shape(dm_in)[1]]) #len(allla)])\n",
    "for c in range(np.shape(dm_in)[1]): #[0:9]:\n",
    "    #print('close?',bnm[bbin[0]],alldm[c]) # make sure closest bin name and dm match closely\n",
    "    # next interpolate to better populate counts array for below rebinning\n",
    "    f = interpolate.interp1d(bnm, dm_in[:,c] ) #allc[:,c])\n",
    "    allci = f(newb) #np.arange(bnm[0],bnm[bnz-1],(bnm[20]-bnm[19])*.1))\n",
    "    allci = ndimage.filters.gaussian_filter1d(allci,sigma=4)\n",
    "    #plt.figure(figsize=[12,8])\n",
    "    #plt.plot(bnm,allc[:,c],label='c')\n",
    "    #plt.plot(newb,allci,label='ci')\n",
    "    #plt.xlim(0.3,2.0)\n",
    "    #plt.legend()\n",
    "    \n",
    "    dif = np.array(alldm[c] - newb) #bnm) # first take dif to find closest bin to dm\n",
    "    bbin = np.where(abs(dif) == np.min(abs(dif)))[0]\n",
    "    xnewl = np.arange(newb[0], newb[bbin[0]], (newb[bbin[0]]-newb[0])/(newbnz*.6) ) ##coord with newbnz\n",
    "    xnewh = np.arange(newb[bbin[0]],newb[-1], (newb[-1]-newb[bbin[0]])/(newbnz*.4))\n",
    "    #xnewl = np.arange(bnm[0], bnm[bbin[0]], (bnm[bbin[0]]-bnm[0])/(newbnz*.75) )\n",
    "    #xnewh = np.arange(bnm[bbin[0]],bnm[bnz-1], (bnm[bnz-1]-bnm[bbin[0]])/(newbnz*.25))\n",
    "    # cram above Dm counts into newbnz/2 bins\n",
    "    #print('new',newb)\n",
    "    #print('low',xnewl)\n",
    "    #print('hi',xnewh)\n",
    "    fnew = interpolate.interp1d(newb,allci)\n",
    "    newc = np.append( fnew(xnewl), fnew(xnewh) )\n",
    "    #print(len(fnew(xnewl)),len(fnew(xnewh)))\n",
    "    #print('hic',fnew(xnewh))\n",
    "    #print('cts old',np.sum(allci),allci[0:120])\n",
    "    #print('cts new',np.size(newc),np.sum(newc),newc)\n",
    "    if np.sum(newc) == 0: \n",
    "        print('zero!?',alldm[c],bbin,np.sum(allci),np.sum(allc[:,c]))\n",
    "        print(xnewl,xnewh)\n",
    "        print('old',allc[:,c])\n",
    "        print('int',np.mean(allci),np.max(allci[:]))\n",
    "        print(xnewl,fnew(xnewl))\n",
    "        #print('new',newc)\n",
    "        newc = dout[:,c-2] # some bullshit workaround\n",
    "    else:\n",
    "        newc *= np.sum(allc[:,c])/np.sum(newc)\n",
    "    #print('cts adj',newc)\n",
    "    #print('sumz',np.sum(newc),np.sum(allc[:,c]))\n",
    "    dout[:,c] = newc[0:newbnz]\n",
    "    \n",
    "plt.show()\n",
    "\n",
    "\n",
    "#allcout = allc / np.sum(allc,axis=0) # normalize in the vertical by counts, integrate to 1.0\n",
    "dallcout = dout / np.sum(dout,axis=0) # normalize in the vertical by counts, integrate to 1.0\n",
    "    \n",
    "np.save('dj/data/dmnormctsV.'+str(newbnz),dallcout) # normalized by dm\n",
    "#np.save('dj/data/normallcts',allcout)\n",
    "#print('do deez match')\n",
    "#print(np.shape(alllo),np.shape(dout))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pcs, varex = pca2(lallcts,npc)\n",
    "#print(bnm[50:60])\n",
    "#print(np.shape(allcts))\n",
    "#print(info(dout))\n",
    "#print(dout[:,0])\n",
    "#print(allc[:,0])\n",
    "#print(dout[:,990])\n",
    "#print(allc[:,990])\n",
    "#r1,r2,r3 = 743, 12345, 19123\n",
    "r1,r2,r3 = 43, 2345, 1912\n",
    "print(alldm[r1],alldm[r2])\n",
    "print(allc[:,r2])\n",
    "plt.figure(figsize=[8,5])\n",
    "exx = np.arange(0,len(allc[:,0]))\n",
    "plt.plot(exx,allc[:,r1],label='r1')\n",
    "plt.plot(exx,allc[:,r2],label='r2')\n",
    "#plt.plot(exx,allc[:,r3],label='r3')\n",
    "plt.legend()\n",
    "plt.figure(figsize=[8,5])\n",
    "plt.plot(np.arange(0,newbnz),dout[:,r1],label='r1d')\n",
    "plt.plot(np.arange(0,newbnz),dout[:,r2],label='r2d')\n",
    "#plt.plot(np.arange(0,newbnz),dout[:,r3],label='r3d')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(np.shape(pcs))\n",
    "#print(pcs[:,0])\n",
    "#print(var\n",
    "samma = np.sum(allc,axis=0)\n",
    "print(np.shape(samma))\n",
    "print(samma[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##f1 = plt.figure(figsize=[8,8])\n",
    "##plt.plot(ex,pcs[:,0],)\n",
    "##print(np.shape(bname),bname)\n",
    "#pcp = pygal.Line(title='PCs',x_title='Size bins',y_title='Counts',x_labels=bname,\\\n",
    "#                    style=DarkSolarizedStyle,legend_at_top=True)\n",
    "##pcp.logarithmic = True\n",
    "##pcp.x_label_major_every = 5\n",
    "##pcp.x_label_rotation = 30\n",
    "##pcp.show_minor_x_labels = False\n",
    "#varst = []\n",
    "#for p in range(npc):\n",
    "#    varst.extend([\"{:.2}\".format(Decimal(varex[p]*100.0))])\n",
    "#print(varst)\n",
    "#pcp.add('PC1: '+varst[0], pcs[:,0])\n",
    "#pcp.add('PC2: '+varst[1], pcs[:,1])\n",
    "#pcp.add('PC3: '+varst[2], pcs[:,2])\n",
    "#pcp.add('PC4: '+varst[3], pcs[:,3])\n",
    "##pcp.add('PC5', pcs[:,4])\n",
    "#pcp.render_to_png('test-inv.png')\n",
    "#\n",
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(pcs[:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(info(shrnkctsn))\n",
    "print(np.shape(shrnkctsn))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
